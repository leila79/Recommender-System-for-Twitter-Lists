{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ttJ6Hgs9yWQi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numba import jit\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Embedding, Input, MaxPooling2D, Conv2D, concatenate, Flatten, Reshape, BatchNormalization\n",
        "from tqdm import tqdm\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import pickle\n",
        "from sklearn.manifold import TSNE\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IhgIJsL6IF0"
      },
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpLjdGFg6Hgs"
      },
      "outputs": [],
      "source": [
        "def add_negative_sample(path, saveas):\n",
        "    data = pd.read_csv(path, names=['uid', 'lid', 'rate'])\n",
        "    data = data[['uid', 'lid', 'rate']]\n",
        "    users = data['uid'].unique()\n",
        "    lists = data['lid'].unique()\n",
        "    # data['rate'] = 1\n",
        "    for u in users:\n",
        "        i = 0\n",
        "        while i < 5:\n",
        "          l = random.choices(lists)\n",
        "          if not (data[['uid', 'lid']].values == [u, l[0]]).all(axis=1).any():\n",
        "                data.loc[len(data)] = [u, l[0], 0]\n",
        "                i += 1\n",
        "    data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "    data.to_csv(saveas, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJ3leBa6DAa"
      },
      "outputs": [],
      "source": [
        "def split_data_train_test(path, name):\n",
        "    test_percentage = 0.2\n",
        "    df = pd.read_csv(path + name)\n",
        "    df = df.sample(frac=1)\n",
        "\n",
        "    df_train, df_test = train_test_split(\n",
        "        df, test_size=test_percentage, random_state=1)\n",
        "\n",
        "\n",
        "    df_train.to_csv(path + 'twitter_train.csv',\n",
        "                    index=False)\n",
        "    df_test.to_csv(path + 'twitter_test.csv',\n",
        "                   index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeJNuf4j6dlY"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(filename):\n",
        "    # read all the tweets\n",
        "    with open(Data_DIR + filename, 'rb') as f:\n",
        "        tweets_dict = pickle.load(f)\n",
        "    \n",
        "    tweets_num = -np.inf\n",
        "    for values in tweets_dict.values():\n",
        "      if tweets_num < len(values):\n",
        "        tweets_num = len(values)\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    for values in tweets_dict.values():\n",
        "        tokenizer.fit_on_texts(values)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print(\"vocab size:\", vocab_size)\n",
        "\n",
        "    with open(Data_DIR + filename + '_tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    max_lens = []\n",
        "    for values in tweets_dict.values():\n",
        "        texts = tokenizer.texts_to_sequences(values)\n",
        "        max_len_texts = max(len(x) for x in texts)\n",
        "        max_lens.append(max_len_texts)\n",
        "\n",
        "    max_len = max(max_lens)\n",
        "    print(\"max len:\", max_len)\n",
        "\n",
        "    with open(Data_DIR + \"datasetInfo.txt\", \"a\") as f:\n",
        "        f.write(\"vocab_size for \" + filename + \": \" + str(vocab_size))\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"max_len for \" + filename + \": \" + str(max_len))\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"tweets_num for \" + filename + \": \" + str(tweets_num))\n",
        "        f.write(\"\\n\")\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szOChibu6h0V"
      },
      "outputs": [],
      "source": [
        "def create_data(t, filepath):\n",
        "  # read all the users tweets\n",
        "  with open(Data_DIR + \"user_tweets\", 'rb') as f:\n",
        "      user_tweets_dict = pickle.load(f)\n",
        "\n",
        "  # read all the lists tweets\n",
        "  with open(Data_DIR + \"list_tweets\", 'rb') as f:\n",
        "      lists_tweets_dict = pickle.load(f)\n",
        "\n",
        "  # read dataset maxlength and tweet_numbers\n",
        "  with open(Data_DIR + \"datasetInfo.txt\", \"r\") as f:\n",
        "          for line in f.readlines():\n",
        "              name, value = line.split(\": \")\n",
        "              type, category = name.split(\" for \")\n",
        "              if type == \"vocab_size\":\n",
        "                if category == \"user_tweets\":\n",
        "                    vocab_size_users = int(value.strip(\"\\n\"))\n",
        "                elif category == \"list_tweets\":\n",
        "                    vocab_size_lists = int(value.strip(\"\\n\"))\n",
        "              if type == \"max_len\":\n",
        "                  if category == \"user_tweets\":\n",
        "                      max_len_users = int(value.strip(\"\\n\"))\n",
        "                  elif category == \"list_tweets\":\n",
        "                      max_len_lists = int(value.strip(\"\\n\"))\n",
        "              elif type == \"tweets_num\":\n",
        "                  if category == \"user_tweets\":\n",
        "                      tweet_number_users = 1000\n",
        "                  elif category == \"list_tweets\":\n",
        "                      tweet_number_lists = 500\n",
        "  f.close()\n",
        "\n",
        "  # read user_list_realtions\n",
        "  df = pd.read_csv(filepath)\n",
        "  print(\"len:\", df.shape)\n",
        "\n",
        "  uids = df['uid'].values\n",
        "\n",
        "  # user tweets\n",
        "  users_tweets = pd.DataFrame(columns=['user_id', 'tweets'])\n",
        "\n",
        "  for id in uids:\n",
        "      if id in user_tweets_dict.keys():\n",
        "          # id wrote user_tweets_dict[id]\n",
        "          users_tweets = pd.concat(\n",
        "              [users_tweets, pd.DataFrame.from_records([{'user_id': id, 'tweets': user_tweets_dict[id]}])])\n",
        "\n",
        "  lids = df['lid'].values\n",
        "  # list tweets\n",
        "  lists_tweets = pd.DataFrame(columns=['list_id', 'tweets'])\n",
        "\n",
        "  for id in lids:\n",
        "      if id in lists_tweets_dict.keys():\n",
        "          # id wrote lists_tweets_dict[id]\n",
        "          lists_tweets = pd.concat(\n",
        "              [lists_tweets, pd.DataFrame.from_records([{'list_id': id, 'tweets': lists_tweets_dict[id]}])])\n",
        "\n",
        "  # tokenizer\n",
        "  with open(Data_DIR + 'user_tweets_tokenizer.pickle', 'rb') as handle:\n",
        "      user_tokenizer = pickle.load(handle)\n",
        "\n",
        "  with open(Data_DIR + 'list_tweets_tokenizer.pickle', 'rb') as handle:\n",
        "      list_tokenizer = pickle.load(handle)\n",
        "\n",
        "  user_trains = []\n",
        "  for values in users_tweets['tweets']:\n",
        "      user_train = user_tokenizer.texts_to_sequences(values)\n",
        "      user_train = pad_sequences(\n",
        "          user_train, padding='post', maxlen=max_len_users)\n",
        "      user_trains.append(user_train)\n",
        "\n",
        "  list_trains = []\n",
        "  for values in lists_tweets['tweets']:\n",
        "      list_train = list_tokenizer.texts_to_sequences(values)\n",
        "      list_train = pad_sequences(\n",
        "          list_train, padding='post', maxlen=max_len_lists)\n",
        "      list_trains.append(list_train)\n",
        "\n",
        "  Y = df['rate'].values\n",
        "  with open( t + \".pkl\", \"wb\") as f:\n",
        "      pickle.dump([user_trains, list_trains, Y, df], f)\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3r1KB8H64lE"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXJOwMYl69LU"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSqecaaH68hW"
      },
      "outputs": [],
      "source": [
        "# tokenizer\n",
        "\n",
        "with open(Data_DIR + 'user_tweets_tokenizer.pickle', 'rb') as handle:\n",
        "      user_tokenizer = pickle.load(handle)\n",
        "\n",
        "with open(Data_DIR + 'list_tweets_tokenizer.pickle', 'rb') as handle:\n",
        "    list_tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMOq4ryB66DR"
      },
      "outputs": [],
      "source": [
        "with open(Data_DIR + \"datasetInfo.txt\", \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            name, value = line.split(\": \")\n",
        "            type, category = name.split(\" for \")\n",
        "            if type == \"vocab_size\":\n",
        "              if category == \"user_tweets\":\n",
        "                  vocab_size_users = int(value.strip(\"\\n\"))\n",
        "              elif category == \"list_tweets\":\n",
        "                  vocab_size_lists = int(value.strip(\"\\n\"))\n",
        "            if type == \"max_len\":\n",
        "                if category == \"user_tweets\":\n",
        "                    max_len_users = int(value.strip(\"\\n\"))\n",
        "                elif category == \"list_tweets\":\n",
        "                    max_len_lists = int(value.strip(\"\\n\"))\n",
        "            elif type == \"tweets_num\":\n",
        "                if category == \"user_tweets\":\n",
        "                    tweet_number_users = 1000\n",
        "                elif category == \"list_tweets\":\n",
        "                    tweet_number_lists = 500\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V2GN7Ec7Lcz"
      },
      "outputs": [],
      "source": [
        "embedding_size = 25\n",
        "filter_sizes = [3, 4]\n",
        "num_filters = 100\n",
        "tweet_num_users = 1000\n",
        "tweet_num_lists = 500\n",
        "input_shape_u = (tweet_num_users, max_len_users)\n",
        "input_shape_l = (tweet_num_lists, max_len_lists)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPkQXZFb7FLc"
      },
      "source": [
        "## GloVe Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu58v8a87H43"
      },
      "outputs": [],
      "source": [
        "embeddings_index = dict()\n",
        "with open(Data_DIR + 'glove.twitter.27B/glove.twitter.27B.25d.txt', 'r', encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype=float)\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovDKrTEG7Pkk"
      },
      "outputs": [],
      "source": [
        "# list embeddings_matrix\n",
        "list_embeddings_matrix = np.zeros((vocab_size_lists, embedding_size))\n",
        "for word, index in list_tokenizer.word_index.items():\n",
        "    if index > vocab_size_lists - 1:\n",
        "        break\n",
        "    else:\n",
        "        embeddings_vector = embeddings_index.get(word)\n",
        "        if embeddings_vector is not None:\n",
        "            list_embeddings_matrix[index] = embeddings_vector\n",
        "\n",
        "# user embeddings_matrix\n",
        "user_embeddings_matrix = np.zeros((vocab_size_users, embedding_size))\n",
        "for word, index in user_tokenizer.word_index.items():\n",
        "    if index > vocab_size_users - 1:\n",
        "        break\n",
        "    else:\n",
        "        embeddings_vector = embeddings_index.get(word)\n",
        "        if embeddings_vector is not None:\n",
        "            user_embeddings_matrix[index] = embeddings_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09cxa6HG7TF7"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJw4Uk4a7ank"
      },
      "outputs": [],
      "source": [
        "from keras import layers, initializers, regularizers, constraints\n",
        "import keras.backend as K\n",
        "def dot_product(x, kernel):\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class FMLayer(layers.Layer):\n",
        "  def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True,n_latent=32, fm_k=8, **kwargs):\n",
        "    self.supports_masking = True\n",
        "    self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "    self.W_regularizer = regularizers.get(W_regularizer)\n",
        "    self.u_regularizer = regularizers.get(u_regularizer)\n",
        "\n",
        "    self.W_constraint = constraints.get(W_constraint)\n",
        "    self.u_constraint = constraints.get(u_constraint)\n",
        "\n",
        "    self.n_latent = n_latent\n",
        "    self.fm_k = fm_k\n",
        "    super(FMLayer, self).__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "\n",
        "    self.W = self.add_weight(shape=(self.n_latent*2, 1),\n",
        "                              initializer=self.init,\n",
        "                              name='{}_W'.format(self.name),\n",
        "                              regularizer=self.W_regularizer,\n",
        "                              constraint=self.W_constraint,\n",
        "                              trainable=True,)\n",
        "    self.b = self.add_weight(\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     trainable=True,)\n",
        "\n",
        "    self.u = self.add_weight(shape=(self.n_latent*2, self.fm_k),\n",
        "                              initializer=self.init,\n",
        "                              name='{}_u'.format(self.name),\n",
        "                              regularizer=self.u_regularizer,\n",
        "                              constraint=self.u_constraint,\n",
        "                              trainable=True,)\n",
        "\n",
        "    super(FMLayer, self).build(input_shape)\n",
        "\n",
        "  def compute_mask(self, input, input_mask=None):\n",
        "      # do not pass the mask to the next layers\n",
        "      return None\n",
        "\n",
        "  def call(self, x, mask=None):\n",
        "      one = K.dot(x, self.W)\n",
        "      inte1 = K.dot(x, self.u)\n",
        "      inte2 = K.dot(K.square(x), K.square(self.u))  \n",
        "      inter=(K.square(inte1)-inte2)*0.5\n",
        "      inter = Dropout(0.5)(inter)\n",
        "      inter=K.sum(inter,1,keepdims=True)\n",
        "      return one + inter + self.b\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "      return input_shape[0], input_shape[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APX8HJpe7QVM"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "with tf.device('/device:GPU:0'):\n",
        "  user_input = Input(shape=input_shape_u, dtype='int32')\n",
        "  #Word embedding (using pre trained word vectors)\n",
        "  embedder = Embedding(vocab_size_users, embedding_size, weights=[\n",
        "                      user_embeddings_matrix],  input_length=tweet_num_users, trainable=True)\n",
        "  user_embed = embedder(user_input)\n",
        "  # print(user_embed.shape)\n",
        "  pool_outputs = []\n",
        "  for filter_size in filter_sizes:\n",
        "    filter_shape = (filter_size, embedding_size)\n",
        "    user_cnn = Conv2D(num_filters, filter_shape, strides=(1, 1), padding='valid',\n",
        "                      data_format='channels_last', activation='relu',kernel_initializer='glorot_normal',\n",
        "                      bias_initializer=keras.initializers.constant(0.1),\n",
        "                      name='convolution_user_{:d}'.format(filter_size))(user_embed)\n",
        "    max_pool_shape = (tweet_num_users - filter_size + 1, 1)\n",
        "    user_pool = MaxPooling2D(pool_size=max_pool_shape,\n",
        "                                      strides=(1, 1), padding='valid',\n",
        "                                      data_format='channels_last',\n",
        "                                      name='max_pooling_user_{:d}'.format(filter_size))(user_cnn)\n",
        "    pool_outputs.append(user_pool)\n",
        "  user_cnn = concatenate(pool_outputs, axis=-1)\n",
        "  user_flat = Flatten()(user_cnn)\n",
        "  user_drop = Dropout(0.5)(user_flat)\n",
        "  user_output = Dense(258)(user_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzLprGil7WyX"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  list_input = Input(shape=input_shape_l, dtype='int32')\n",
        "  #Word embedding (using pre trained word vectors)\n",
        "  list_embedder = Embedding(vocab_size_lists, embedding_size, weights=[\n",
        "      list_embeddings_matrix], input_length=tweet_num_lists, trainable=True)\n",
        "  list_embed = list_embedder(list_input)\n",
        "  pool_outputs = []\n",
        "  for filter_size in filter_sizes:\n",
        "    filter_shape = (filter_size, embedding_size)\n",
        "    list_cnn = Conv2D(num_filters, filter_shape, strides=(1, 1), padding='valid',\n",
        "                      data_format='channels_last', activation='relu',kernel_initializer='glorot_normal',\n",
        "                      bias_initializer=keras.initializers.constant(0.1),\n",
        "                      name='convolution_list_{:d}'.format(filter_size))(list_embed)\n",
        "    max_pool_shape = (tweet_num_lists - filter_size + 1, 1)\n",
        "    list_pool = MaxPooling2D(pool_size=max_pool_shape,\n",
        "                                      name='max_pooling_list_{:d}'.format(filter_size))(list_cnn)\n",
        "    pool_outputs.append(list_pool)\n",
        "  list_cnn = concatenate(pool_outputs, axis=-1)\n",
        "  list_flat = Flatten()(list_cnn)\n",
        "  list_drop = Dropout(0.5)(list_flat)\n",
        "  list_output = Dense(258)(list_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHAi7KI67cWU"
      },
      "outputs": [],
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  concat = concatenate([user_output, list_output], axis=-1)\n",
        "  activate = tf.keras.layers.ReLU()(concat)\n",
        "  main_output = FMLayer(n_latent=258)(activate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15fEpQWF7hJ8"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=[user_input, list_input], outputs=main_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEOPbdN87kXG"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smD6cV8I7l_H"
      },
      "outputs": [],
      "source": [
        "u, l, y, df = pd.read_pickle(Data_DIR + \"train.pkl\")\n",
        "u = pad_sequences(u, padding='post', maxlen=tweet_num_users)\n",
        "l = pad_sequences(l, padding='post', maxlen=tweet_num_lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG--txt37uU8"
      },
      "outputs": [],
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "    return K.sum((K.cast(y_true, K.floatx()) - y_pred)**2) / 2\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "\treturn K.sqrt(K.mean(K.square(y_pred - K.cast(y_true, K.floatx())), axis=-1))\n",
        " \n",
        "def mae(y_true, y_pred):\n",
        "\treturn K.mean(K.abs(y_pred - K.cast(y_true, K.floatx())), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLRmjJFwJVZF",
        "outputId": "f540bee8-0063-466c-eb55-cc25d2da1357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "149/149 [==============================] - 286s 2s/step - loss: 5830375.0000 - rmse: 35.6318 - mae: 35.6318 - accuracy: 0.4974 - val_loss: 36.7141 - val_rmse: 0.7266 - val_mae: 0.7266 - val_accuracy: 0.3738\n",
            "Epoch 2/10\n",
            "149/149 [==============================] - 263s 2s/step - loss: 61.7370 - rmse: 0.6894 - mae: 0.6894 - accuracy: 0.5920 - val_loss: 11.7141 - val_rmse: 0.3842 - val_mae: 0.3842 - val_accuracy: 0.6583\n",
            "Epoch 3/10\n",
            "149/149 [==============================] - 262s 2s/step - loss: 11.8659 - rmse: 0.4060 - mae: 0.4060 - accuracy: 0.6546 - val_loss: 10.1032 - val_rmse: 0.3797 - val_mae: 0.3797 - val_accuracy: 0.6766\n",
            "Epoch 4/10\n",
            "149/149 [==============================] - 262s 2s/step - loss: 10.7831 - rmse: 0.3880 - mae: 0.3880 - accuracy: 0.6846 - val_loss: 9.9050 - val_rmse: 0.3897 - val_mae: 0.3897 - val_accuracy: 0.6796\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6cf8fd3850>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1 = Model(inputs=[user_input, list_input], outputs=main_output)\n",
        "model1.compile(loss=loss_fn,\n",
        "              optimizer=tf.keras.optimizers.RMSprop(), metrics=[rmse, mae, 'accuracy'])\n",
        "early_stopping = EarlyStopping(monitor='rmse', patience=3, mode='max')\n",
        "model1.fit([u, l], y, batch_size=100, callbacks=[early_stopping],\n",
        "          epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQgVHIH974Nd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_history(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['rmse'])\n",
        "    plt.plot(history.history['val_rmse'])\n",
        "    plt.title('Model RMSE')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history.history['mae'])\n",
        "    plt.plot(history.history['val_mae'])\n",
        "    plt.title('Model MAE')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V83zmjHl7-au"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2wVei--79a7"
      },
      "outputs": [],
      "source": [
        "data = df[['uid', 'lid', 'rate']]\n",
        "users = data['uid'].unique()\n",
        "lists = data['lid'].unique()\n",
        "i = 6\n",
        "max = len(users)\n",
        "hits = []\n",
        "while i < max:\n",
        "  user = users[i]\n",
        "  test = pd.DataFrame(columns=['uid', 'lid', 'rate'])\n",
        "  for l in lists:\n",
        "    if (data[['uid', 'lid', 'rate']].values == [user, l, 1]).all(axis=1).any():\n",
        "      test.loc[len(test)] = [user, l, 1]\n",
        "    else:\n",
        "      test.loc[len(test)] = [user, l, 0]\n",
        "  test = test.sample(frac=1).reset_index(drop=True)\n",
        "  test.to_csv(Data_DIR + \"Tests/twitter_test_\" + str(user) + \".csv\", index=False)\n",
        "  create_data(Data_DIR + \"test\", \n",
        "            Data_DIR + \"Tests/twitter_test_\"+ str(user) +\".csv\")\n",
        "  ut, lt, yt, dft = pd.read_pickle(Data_DIR + \"test.pkl\")\n",
        "  if dft['uid'][0] != user:\n",
        "    print(\"wrong id ... \", dft['uid'][0])\n",
        "    break\n",
        "  ut = pad_sequences(ut, padding='post', maxlen=tweet_num_users)\n",
        "  lt = pad_sequences(lt, padding='post', maxlen=tweet_num_lists)\n",
        "  predictions = model1.predict([ut, lt], batch_size=100, verbose=1)\n",
        "  dft['prediction'] = np.array(predictions)\n",
        "  sorted = dft.sort_values(by=['prediction'], ascending=False).reset_index(drop=True)\n",
        "  sorted.to_csv(Data_DIR + \"Predictions/output_\" + str(user) + \".csv\")\n",
        "  indexs = sorted.loc[sorted['rate'] == 1].index\n",
        "  count = 0\n",
        "  for idx in indexs:\n",
        "    if idx < 15 :\n",
        "      count += 1\n",
        "      hits.append(1)\n",
        "    else:\n",
        "      hits.append(0)\n",
        "  recall = count / len(indexs)\n",
        "  print(user, \", \", indexs, \" ==> \", recall)\n",
        "  i+=1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
